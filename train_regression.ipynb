{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMDLFMnqOFGKKgNzNN+k3JR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GZR1996/garment-detection/blob/master/train_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssdD58qoyeDz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "acc27022-8198-496e-fbc5-f8b605dc33eb"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "ROOT_DIR = os.path.abspath(os.path.dirname('.'))\n",
        "!git clone https://GZR1996:asas6765@github.com/GZR1996/garment-detection.git \n",
        "os.chdir('./garment-detection')\n",
        "sys.path.append(os.path.join(ROOT_DIR, 'garment-detection'))\n",
        "!mkdir ./simulation/data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'garment-detection'...\n",
            "remote: Enumerating objects: 288, done.\u001b[K\n",
            "remote: Counting objects: 100% (288/288), done.\u001b[K\n",
            "remote: Compressing objects: 100% (188/188), done.\u001b[K\n",
            "remote: Total 288 (delta 141), reused 233 (delta 89), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (288/288), 1.24 MiB | 4.39 MiB/s, done.\n",
            "Resolving deltas: 100% (141/141), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg0tqEJ7zqRL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7d38586f-7281-4b20-815d-6e1b7abd1ead"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5u1_o4S0vuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Parser:\n",
        "  def __init__(self, batch_size, epochs, checkpoint_dir, rgb_dir, data_dir,\n",
        "               data_type, sample_dir, train_label_dir, test_label_dir, \n",
        "               validate_label_dir, reload, generate_sample):\n",
        "    self.batch_size = batch_size\n",
        "    self.epochs = epochs\n",
        "    self.checkpoint_dir = checkpoint_dir\n",
        "    self.rgb_dir = rgb_dir\n",
        "    self.data_dir = data_dir\n",
        "    self.data_type = data_type\n",
        "    self.sample_dir = sample_dir\n",
        "    self.train_label_dir = train_label_dir\n",
        "    self.test_label_dir = test_label_dir\n",
        "    self.validate_label_dir = validate_label_dir\n",
        "    self.reload = reload\n",
        "    self.generate_sample = generate_sample "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOfJQccQvxk_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "cellView": "both",
        "outputId": "e1865b93-e1ee-4a03-b08a-c90cea539444"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from models.conv_vae import ConvVAE\n",
        "from utils.garment_dataset import GarmentDataset\n",
        "from utils import utils\n",
        "\n",
        "# Constants\n",
        "from utils.utils import EarlyStopping\n",
        "\n",
        "DIRECTORY = os.path.abspath(os.path.dirname('.'))\n",
        "CHECKPOINT_DIR = os.path.join(DIRECTORY, 'checkpoint', 'vae')\n",
        "DATA_DIR = os.path.join(DIRECTORY, 'simulation', 'data', 'bin')\n",
        "RGB_DIR = os.path.join(DIRECTORY, 'simulation', 'data', 'rgb')\n",
        "SAMPLE_DIR = os.path.join(DIRECTORY, 'simulation', 'data', 'sample')\n",
        "TRAIN_LABEL_DIR = os.path.join(DIRECTORY, 'simulation', 'data', 'vae', 'train_label.csv')\n",
        "TEST_LABEL_DIR = os.path.join(DIRECTORY, 'simulation', 'data', 'vae', 'test_label.csv')\n",
        "VALIDATE_LABEL_DIR = os.path.join(DIRECTORY, 'simulation', 'data', 'vae', 'validate_label.csv')\n",
        "ALL_DIR = os.path.join(DIRECTORY, 'simulation', 'data', 'all.csv')\n",
        "\n",
        "# parameters of training\n",
        "# parser = argparse.ArgumentParser(description='Parameter of train_vae.py')\n",
        "# parser.add_argument('--batch_size', type=int, default=32, help='The number of batches')\n",
        "# parser.add_argument('--epochs', type=int, default=10, help='The number of epochs for training and testing')\n",
        "# parser.add_argument('--checkpoint_dir', type=str, default=CHECKPOINT_DIR, help='Path to vae folder')\n",
        "# parser.add_argument('--rgb_dir', type=str, default=RGB_DIR, help='Path to img folder')\n",
        "# parser.add_argument('--data_dir', type=str, default=DATA_DIR, help='Path to data folder')\n",
        "# parser.add_argument('--data_type', type=str, choices=['rgb', 'raw_depth', 'depth', 'segmentation'], default='depth',\n",
        "#                     help='The data type of data')\n",
        "# parser.add_argument('--sample_dir', type=str, default=SAMPLE_DIR, help='Path to sample folder')\n",
        "# parser.add_argument('--train_label_dir', type=str, default=TRAIN_LABEL_DIR, help='Path to train label')\n",
        "# parser.add_argument('--test_label_dir', type=str, default=TEST_LABEL_DIR, help='Path to test label')\n",
        "# parser.add_argument('--validate_label_dir', type=str, default=VALIDATE_LABEL_DIR, help='Path to test label')\n",
        "# parser.add_argument('--reload', type=int, default=1, choices=[0, 1],\n",
        "#                     help='If true and previous vae exists, reload the best vae')\n",
        "# parser.add_argument('--generate_sample', type=int, choices=[0, 1], default=0,\n",
        "#                     help='If true, the model will not be trained and only generate samples')\n",
        "# args = parser.parse_args()\n",
        "args = Parser(32, 10, CHECKPOINT_DIR, RGB_DIR, DATA_DIR, 'depth', SAMPLE_DIR, \n",
        "                TRAIN_LABEL_DIR, TEST_LABEL_DIR, VALIDATE_LABEL_DIR, 1, 0)\n",
        "\n",
        "if not os.path.exists('./checkpoint'):\n",
        "    os.mkdir('./checkpoint')\n",
        "if not os.path.exists(args.checkpoint_dir):\n",
        "    os.mkdir(args.checkpoint_dir)\n",
        "if not os.path.exists(args.sample_dir):\n",
        "    os.mkdir(args.sample_dir)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.manual_seed(1000)\n",
        "\n",
        "\n",
        "def loss_function(recon_x, x, mu, log_sigma):\n",
        "    \"\"\"\n",
        "    VAE loss function\n",
        "    :param recon_x: reconstructed image\n",
        "    :param x: image\n",
        "    :param mu: mean of latent factor\n",
        "    :param log_sigma: standard deviation of latent factor\n",
        "    :return: loss\n",
        "    \"\"\"\n",
        "    # MSE for batch\n",
        "    # recon_x = recon_x.view([-1, 1, 256, 256])\n",
        "    BCE = F.mse_loss(recon_x, x, size_average=False)\n",
        "    # KL divergence\n",
        "    KLD = -0.5 * torch.sum(1 + 2 * log_sigma - mu.pow(2) - (2 * log_sigma).exp())\n",
        "    loss = BCE + KLD\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def train(model, loader, epoch):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    for batch, data in enumerate(loader):\n",
        "        sample = data['sample'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, sigma = model(sample)\n",
        "        loss = loss_function(recon_batch, sample, mu, sigma)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch % 20 == 0:\n",
        "            print(\n",
        "                'Train epoch: {}, batch: {}, loss: {}, time: {}'.format(epoch, batch, loss, time.time() - epoch_start))\n",
        "\n",
        "    avg_loss = train_loss / len(loader.dataset)\n",
        "    print('Finish training epoch {}, average loss: {}, in {} seconds'.format(epoch,\n",
        "                                                                             avg_loss,\n",
        "                                                                             time.time() - epoch_start))\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def test(model, loader, epoch, is_save=False):\n",
        "    \"\"\" Test the model \"\"\"\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, data in enumerate(loader):\n",
        "            sample = data['sample'].to(device)\n",
        "            recon_batch, mu, sigma = model(sample)\n",
        "            test_loss += loss_function(recon_batch, sample, mu, sigma)\n",
        "            if is_save:\n",
        "                if batch % 20 == 0:\n",
        "                    print('Saving {} batch images in {} seconds'.format(batch, time.time() - epoch_start))\n",
        "                images = recon_batch.view([-1, 256, 256])\n",
        "                utils.save_image(args.sample_dir, np.asarray(images.to('cpu')), np.asarray(data['label']))\n",
        "\n",
        "    avg_loss = test_loss / len(loader.dataset)\n",
        "    if not is_save:\n",
        "        print('Finish testing epoch {}, average loss: {}, in {} seconds'.format(epoch,\n",
        "                                                                                avg_loss,\n",
        "                                                                                time.time() - epoch_start))\n",
        "    else:\n",
        "        print('Finish generate final result at', args.sample_dir)\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "data_transforms = transforms.Compose([transforms.RandomResizedCrop(256),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "train_dataset = GarmentDataset(args.rgb_dir, args.data_dir, args.data_type, args.train_label_dir)\n",
        "test_dataset = GarmentDataset(args.rgb_dir, args.data_dir, args.data_type, args.test_label_dir)\n",
        "validate_dataset = GarmentDataset(args.rgb_dir, args.data_dir, args.data_type, args.validate_label_dir)\n",
        "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=16)\n",
        "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=True, num_workers=16)\n",
        "validate_loader = DataLoader(validate_dataset, batch_size=args.batch_size, shuffle=True, num_workers=16)\n",
        "\n",
        "vae = ConvVAE(img_channels=utils.DATA_SIZE, latent_size=utils.LATENT_SIZE).to(device)\n",
        "optimizer = optim.Adam(vae.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "earlystopping = EarlyStopping('min', patience=30)\n",
        "\n",
        "checkpoint_count = len(os.listdir(args.checkpoint_dir))\n",
        "reload_dir = os.path.join(args.checkpoint_dir, utils.BEST_FILENAME)\n",
        "if args.generate_sample == 0 and args.reload == 1 and os.path.exists(reload_dir):\n",
        "    best_state = torch.load(reload_dir)\n",
        "    print('Reloading vae......, file: ', reload_dir)\n",
        "    vae.load_state_dict(best_state['state_dict'])\n",
        "    optimizer.load_state_dict(best_state['optimizer_dict'])\n",
        "    scheduler.load_state_dict(best_state['scheduler_dict'])\n",
        "    earlystopping.load_state_dict(best_state['earlystopping_dict'])\n",
        "    # delete useless parameter to get more gpu memory\n",
        "    del best_state\n",
        "\n",
        "# generate result\n",
        "if args.generate_sample == 1:\n",
        "    reload_dir = os.path.join(args.checkpoint_dir, utils.BEST_FILENAME)\n",
        "    best_state = torch.load(reload_dir)\n",
        "    print('Loading the best vae......')\n",
        "    print('Start generate samples......')\n",
        "    vae.load_state_dict(best_state['state_dict'])\n",
        "    # delete useless parameter to get more gpu memory\n",
        "    del best_state\n",
        "    all_dataset = GarmentDataset(args.rgb_dir, args.data_dir, args.data_type, ALL_DIR)\n",
        "    all_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=16)\n",
        "    test(vae, all_loader, 0, is_save=True)\n",
        "    os._exit(0)\n",
        "\n",
        "best_loss = None\n",
        "for epoch in range(args.epochs):\n",
        "    train_loss = train(vae, train_loader, checkpoint_count)\n",
        "    scheduler.step(train_loss)\n",
        "    earlystopping.step(train_loss)\n",
        "    test_loss = test(vae, test_loader, checkpoint_count)\n",
        "    is_best = not best_loss or test_loss < best_loss\n",
        "    if is_best:\n",
        "        best_loss = test_loss\n",
        "\n",
        "    loss_state = {'epoch': checkpoint_count,\n",
        "                  'train_loss': train_loss,\n",
        "                  'test_loss': test_loss}\n",
        "    best_state = {'epoch': checkpoint_count,\n",
        "                  'state_dict': vae.state_dict(),\n",
        "                  'encoder_dict': vae.encoder.state_dict(),\n",
        "                  'optimizer_dict': optimizer.state_dict(),\n",
        "                  'scheduler_dict': scheduler.state_dict(),\n",
        "                  'earlystopping_dict': earlystopping.state_dict(),\n",
        "                  'train_loss': train_loss,\n",
        "                  'test_loss': test_loss,\n",
        "                  'best_loss': best_loss}\n",
        "    checkpoint_name = os.path.join(args.checkpoint_dir, str(checkpoint_count) + '.pth')\n",
        "    utils.save_checkpoint(loss_state, best_state, is_best, checkpoint_name, reload_dir)\n",
        "    checkpoint_count += 1\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-9c4c25064ef4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m                                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGarmentDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgb_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_label_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGarmentDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgb_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_label_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0mvalidate_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGarmentDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgb_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_label_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/garment-detection/garment-detection/utils/garment_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rgb_dir, data_dir, data_type, csv_dir, transform)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File /content/garment-detection/garment-detection/garment-detection/simulation/data/vae/train_label.csv does not exist: '/content/garment-detection/garment-detection/garment-detection/simulation/data/vae/train_label.csv'"
          ]
        }
      ]
    }
  ]
}